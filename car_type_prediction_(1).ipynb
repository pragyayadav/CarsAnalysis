{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "car_type_prediction (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragyayadav/CarsAnalysis/blob/main/car_type_prediction_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "drYZwFpyP3Q3",
        "outputId": "7ecaa853-68dc-449a-b5b1-67e8b7bbd79a"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "#Import a bunch of stuff\n",
        "import numpy as np\n",
        "from keras import layers, callbacks\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "#from keras.utils import plot_model\n",
        "#from resnets_utils import *\n",
        "from keras.initializers import glorot_uniform\n",
        "import dask.array as da\n",
        "import scipy.misc\n",
        "import scipy.io\n",
        "from scipy import misc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from matplotlib.pyplot import imshow\n",
        "#This only works on jupyter notebook:\n",
        "#get_ipython().magic('matplotlib inline')\n",
        "\n",
        "#Clear Memory\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "LOCAL_SOURCE='/bin/Car/traindata'\n",
        "OUTPUT_PATH='/bin/Car/trainoutput'\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)\n",
        "\n",
        "HEIGHT = 224\n",
        "WIDTH = 224\n",
        "CHANNELS = 3\n",
        "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
        "#h5_train_proc = { \"fname\": \"/bin/Car/traindata/cars.h5\", \"imgDir\": \"/bin/Car/traindata\", \"matFile\": \"cars_test_annos.mat\"}\n",
        "h5_train_proc = { \"fname\": \"/bin/Car/traindata/cars.h5\", \"imgDir\": \"/bin/Car/traindata\", \"matFile\": \"cars_train_annos.mat\"}\n",
        "#h5_test_proc = { \"fname\": \"test_cars.h5\", \"imgDir\": \"cars_test\", \"matFile\": \"cars_test_annos.mat\"}\n",
        "h5_test_proc = { \"fname\": \"/bin/Car/testdata/test_cars.h5\", \"imgDir\": \"/bin/Car/testdata\", \"matFile\": \"cars_test_annos.mat\"}\n",
        "for test_train in [h5_train_proc, h5_test_proc]:\n",
        "  \n",
        "  cars_annos = scipy.io.loadmat( LOCAL_SOURCE + '/devkit/{}'.format(test_train.get('matFile')) )\n",
        "  \n",
        "  car_bbox_x1 = np.zeros(0,)\n",
        "  car_bbox_x2 = np.zeros(0,)\n",
        "  car_bbox_y1 = np.zeros(0,)\n",
        "  car_bbox_y2 = np.zeros(0,)\n",
        "  car_class = np.zeros(0,)\n",
        "  car_fname = np.zeros(0,)\n",
        "  print(\"hi\",car_fname.max)\n",
        "  \n",
        "  NUM_IMAGES = len(cars_annos['annotations'][0])\n",
        "  \n",
        "  car_image = np.zeros((NUM_IMAGES, HEIGHT, WIDTH, CHANNELS))\n",
        "  \n",
        "  # Iterating through the annotations and loading images and labels\n",
        "  i = 0\n",
        " #for car in cars_annos['annotations'][0]:\n",
        "    car_bbox_x1 =  np.append(car_bbox_x1, car[0][0].item())\n",
        "    car_bbox_x2 = np.append(car_bbox_x2, car[1][0].item())\n",
        "    car_bbox_y1 = np.append(car_bbox_y1, car[2][0].item())\n",
        "    car_bbox_y2 = np.append(car_bbox_y2, car[3][0].item())\n",
        "    car_class = np.append(car_class, car[4][0].item())\n",
        "    car_fname = np.append(car_fname, car[5][0].item())\n",
        "\n",
        "    image_location =LOCAL_SOURCE + '/{}/{}'.format(test_train.get('imgDir'), car[5][0].item())\n",
        "    image_tmp = cv2.imread(LOCAL_SOURCE + '/{}/{}'.format(test_train.get('imgDir'), car[5][0].item()))\n",
        "    car_image[i][:][:][:] = cv2.resize(image_tmp, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC).astype(int)\n",
        "    i = i + 1\n",
        "\n",
        "\n",
        "#==========================================================================\n",
        "#PART 0 - Set Some Stuff\n",
        "#==========================================================================\n",
        "img_width = 224\n",
        "img_height = 224\n",
        "classes = 196\n",
        "batch_size = 16\n",
        "epochs = 10 #Be sure to set this based on how long you want to run for!\n",
        "patience = 50 #For Callbacks\n",
        "verbose = 1\n",
        "num_train_samples = 6549\n",
        "num_valid_samples = 1695 #Cross validation: num_train_samples + num_valid_samples = # of train images\n",
        "mode = 'sgd' #adam or sgd\n",
        "\n",
        "#==========================================================================\n",
        "#PART 1 - Initialize the Model\n",
        "#==========================================================================\n",
        "\n",
        "#Import the structure for an identity block, a convolution block, and a full residual CNN (based on both):\n",
        "from identity_block import *\n",
        "from conv_block import *\n",
        "from res_net import *\n",
        "\n",
        "#Build the model:\n",
        "model = ResNet(input_shape = (img_width, img_height, 3), classes = classes)\n",
        "if (mode == 'sgd'): optimizer = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True) #This one seems to work better\n",
        "if (mode == 'adam'): optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "try:\n",
        "\tmodel.load_weights(\"weights.best.hdf5\")\n",
        "    #model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "except:\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#==========================================================================\n",
        "#PART 2 - Import Data and Set-up Training Parameters (Callbacks, etc.)\n",
        "#==========================================================================\n",
        "\n",
        "# Load Dataset (note that test data is loaded separately lower down - this helps with memory)\n",
        "X_train, Y_train_orig, classes = load_train_dataset()\n",
        "\n",
        "# It's not necessary to normalize X_train because of the Scale step in the neural network (after BatchNorm)\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "Y_train = convert_to_one_hot(Y_train_orig, 196).T\n",
        "\n",
        "# If cross validation is desired, then take some of the training data for this purpose\n",
        "if (num_valid_samples > 0):\n",
        "\tX_valid = X_train[num_train_samples::,:,:,:]\n",
        "\tY_valid = Y_train[num_train_samples::]\n",
        "\tX_train = X_train[0:num_train_samples,:,:,:]\n",
        "\tY_train = Y_train[0:num_train_samples]\n",
        "\t\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_valid shape: \" + str(X_valid.shape))\n",
        "print (\"Y_valid shape: \" + str(Y_valid.shape))\n",
        "\n",
        "# Data Augmentation and Cross Validation\n",
        "train_data_gen = image.ImageDataGenerator(rotation_range=20., width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2, horizontal_flip=True)\n",
        "train_data_gen.fit(X_train)\n",
        "train_generator = train_data_gen.flow(X_train, Y_train, batch_size=batch_size)\n",
        "valid_data_gen = image.ImageDataGenerator()\n",
        "valid_generator = valid_data_gen.flow(X_valid, Y_valid, batch_size=batch_size)\n",
        "\n",
        "# Callbacks\n",
        "checkpoint = callbacks.ModelCheckpoint(\"weights.best.hdf5\", monitor='val_acc', verbose=verbose, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "csv_logger = callbacks.CSVLogger('training.log', append=True)\n",
        "early_stop = callbacks.EarlyStopping('val_acc', patience=patience)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau('val_acc', factor=0.1, patience=int(patience/4), verbose=1)\n",
        "callbacks=[csv_logger,checkpoint,early_stop,reduce_lr]\n",
        "\n",
        "#==========================================================================\n",
        "#PART 3 - Train the Model\n",
        "#==========================================================================\n",
        "model.fit_generator(train_generator, steps_per_epoch=num_train_samples/batch_size, validation_data=valid_generator, validation_steps=num_valid_samples/batch_size, epochs=epochs, callbacks=callbacks, verbose=verbose)\n",
        "\n",
        "#==========================================================================\n",
        "#PART 4 - Test the Model\n",
        "#==========================================================================\n",
        "\n",
        "#Tidy up memory:\n",
        "X_train_orig = Y_train_orig = X_train = Y_train = None\n",
        "\n",
        "#Imoprt stuff\n",
        "X_test, Y_test_orig, classes = load_test_dataset()\n",
        "Y_test = convert_to_one_hot(Y_test_orig, 196).T\n",
        "\n",
        "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))\n",
        "\n",
        "preds = model.evaluate(X_test, Y_test)\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:400: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/bin/Car/traindata/devkit/cars_test_annos.mat'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-51f490c80101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtest_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mh5_train_proc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5_test_proc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0mcars_annos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mLOCAL_SOURCE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/devkit/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matFile'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0mcar_bbox_x1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \"\"\"\n\u001b[1;32m    215\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopened\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reader needs file name or open file-like object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/bin/Car/traindata/devkit/cars_test_annos.mat'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM_NRB5th59p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}