{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CarModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODXrp+S0ttHQyM0hgbmZEK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragyayadav/CarsAnalysis/blob/main/CarModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "wWjz1DwLpPBD",
        "outputId": "79dd73d1-7f54-412a-8b2b-d141b633e1d5"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "#Import a bunch of stuff\n",
        "import numpy as np\n",
        "from keras import layers, callbacks\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "from resnets_utils import *\n",
        "from keras.initializers import glorot_uniform\n",
        "import dask.array as da\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "#This only works on jupyter notebook:\n",
        "#get_ipython().magic('matplotlib inline')\n",
        "\n",
        "#Clear Memory\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)\n",
        "\n",
        "#==========================================================================\n",
        "#PART 0 - Set Some Stuff\n",
        "#==========================================================================\n",
        "img_width = 224\n",
        "img_height = 224\n",
        "classes = 196\n",
        "batch_size = 16\n",
        "epochs = 10 #Be sure to set this based on how long you want to run for!\n",
        "patience = 50 #For Callbacks\n",
        "verbose = 1\n",
        "num_train_samples = 6549\n",
        "num_valid_samples = 1695 #Cross validation: num_train_samples + num_valid_samples = # of train images\n",
        "mode = 'sgd' #adam or sgd\n",
        "\n",
        "#==========================================================================\n",
        "#PART 1 - Initialize the Model\n",
        "#==========================================================================\n",
        "\n",
        "#Import the structure for an identity block, a convolution block, and a full residual CNN (based on both):\n",
        "from identity_block import *\n",
        "from conv_block import *\n",
        "from res_net import *\n",
        "\n",
        "#Build the model:\n",
        "model = ResNet(input_shape = (img_width, img_height, 3), classes = classes)\n",
        "if (mode == 'sgd'): optimizer = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True) #This one seems to work better\n",
        "if (mode == 'adam'): optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "try:\n",
        "\tmodel.load_weights(\"weights.best.hdf5\")\n",
        "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "except:\n",
        "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#==========================================================================\n",
        "#PART 2 - Import Data and Set-up Training Parameters (Callbacks, etc.)\n",
        "#==========================================================================\n",
        "\n",
        "# Load Dataset (note that test data is loaded separately lower down - this helps with memory)\n",
        "X_train, Y_train_orig, classes = load_train_dataset()\n",
        "\n",
        "# It's not necessary to normalize X_train because of the Scale step in the neural network (after BatchNorm)\n",
        "\n",
        "# Convert training and test labels to one hot matrices\n",
        "Y_train = convert_to_one_hot(Y_train_orig, 196).T\n",
        "\n",
        "# If cross validation is desired, then take some of the training data for this purpose\n",
        "if (num_valid_samples > 0):\n",
        "\tX_valid = X_train[num_train_samples::,:,:,:]\n",
        "\tY_valid = Y_train[num_train_samples::]\n",
        "\tX_train = X_train[0:num_train_samples,:,:,:]\n",
        "\tY_train = Y_train[0:num_train_samples]\n",
        "\t\n",
        "\n",
        "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
        "print (\"X_train shape: \" + str(X_train.shape))\n",
        "print (\"Y_train shape: \" + str(Y_train.shape))\n",
        "print (\"X_valid shape: \" + str(X_valid.shape))\n",
        "print (\"Y_valid shape: \" + str(Y_valid.shape))\n",
        "\n",
        "# Data Augmentation and Cross Validation\n",
        "train_data_gen = image.ImageDataGenerator(rotation_range=20., width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2, horizontal_flip=True)\n",
        "train_data_gen.fit(X_train)\n",
        "train_generator = train_data_gen.flow(X_train, Y_train, batch_size=batch_size)\n",
        "valid_data_gen = image.ImageDataGenerator()\n",
        "valid_generator = valid_data_gen.flow(X_valid, Y_valid, batch_size=batch_size)\n",
        "\n",
        "# Callbacks\n",
        "checkpoint = callbacks.ModelCheckpoint(\"weights.best.hdf5\", monitor='val_acc', verbose=verbose, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "csv_logger = callbacks.CSVLogger('training.log', append=True)\n",
        "early_stop = callbacks.EarlyStopping('val_acc', patience=patience)\n",
        "reduce_lr = callbacks.ReduceLROnPlateau('val_acc', factor=0.1, patience=int(patience/4), verbose=1)\n",
        "callbacks=[csv_logger,checkpoint,early_stop,reduce_lr]\n",
        "\n",
        "#==========================================================================\n",
        "#PART 3 - Train the Model\n",
        "#==========================================================================\n",
        "model.fit_generator(train_generator, steps_per_epoch=num_train_samples/batch_size, validation_data=valid_generator, validation_steps=num_valid_samples/batch_size, epochs=epochs, callbacks=callbacks, verbose=verbose)\n",
        "\n",
        "#==========================================================================\n",
        "#PART 4 - Test the Model\n",
        "#==========================================================================\n",
        "\n",
        "#Tidy up memory:\n",
        "X_train_orig = Y_train_orig = X_train = Y_train = None\n",
        "\n",
        "#Imoprt stuff\n",
        "X_test, Y_test_orig, classes = load_test_dataset()\n",
        "Y_test = convert_to_one_hot(Y_test_orig, 196).T\n",
        "\n",
        "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
        "print (\"X_test shape: \" + str(X_test.shape))\n",
        "print (\"Y_test shape: \" + str(Y_test.shape))\n",
        "\n",
        "preds = model.evaluate(X_test, Y_test)\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d62277ea3332>\"\u001b[0;36m, line \u001b[0;32m62\u001b[0m\n\u001b[0;31m    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\u001b[0m\n\u001b[0m                                                                                             ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaRP8tscqoSh"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}